{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Request\n",
    "from fastapi.responses import HTMLResponse, JSONResponse\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from fastapi.templating import Jinja2Templates\n",
    "from pydantic import BaseModel\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from uvicorn import run\n",
    "app = FastAPI()\n",
    "\n",
    "import asyncio\n",
    "import uvicorn\n",
    "\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from trt_inference_api import CustomLLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/adebolajo/Desktop/trt-llm/mistral-7b-int4-chat_1.2/trt_engines\n",
      "/home/adebolajo/Desktop/trt-llm/mistral-7b-int4-chat_1.2/mistral7b_hf_tokenizer\n"
     ]
    }
   ],
   "source": [
    "token_path = \"/home/adebolajo/Desktop/trt-llm/mistral-7b-int4-chat_1.2/mistral7b_hf_tokenizer\"\n",
    "engine_path = \"/home/adebolajo/Desktop/trt-llm/mistral-7b-int4-chat_1.2/trt_engines\"\n",
    "\n",
    "llm = CustomLLM(n=5,tokenizer_dir = token_path, engine_dir = engine_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adebolajo/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Notice that \"chat_history\" is present in the prompt template\n",
    "template = \"\"\"You are a nice chatbot having a conversation with a human.\n",
    "\n",
    "Previous conversation:\n",
    "{chat_history}\n",
    "\n",
    "New human question: {question}\n",
    "Response:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "# Notice that we need to align the `memory_key`\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Message(BaseModel):\n",
    "    content: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [16195]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:50016 - \"GET / HTTP/1.1\" 200 OK\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a nice chatbot having a conversation with a human.\n",
      "\n",
      "Previous conversation:\n",
      "Human: hi \n",
      "AI:  Hello! How can I help you today?\n",
      "\n",
      "Human: what happened to the world in 2020\n",
      "AI:  \n",
      "\n",
      "In 2020, the world experienced a pandemic caused by the novel coronavirus (COVID-19). The virus spread rapidly and caused widespread illness and death. Many countries implemented lockdowns and other measures to try to slow the spread of the virus. The pandemic also had a significant impact on the global economy, causing widespread job losses and business closures. Additionally, there were protests and unrest in many parts of the world, including the United States, where there were widespread protests against the killing of George Floyd.\n",
      "\n",
      "\n",
      "New human question: hi\n",
      "Response:\u001b[0m\n",
      "Input [Text 0]: \"<s> You are a nice chatbot having a conversation with a human.\n",
      "\n",
      "Previous conversation:\n",
      "Human: hi \n",
      "AI:  Hello! How can I help you today?\n",
      "\n",
      "Human: what happened to the world in 2020\n",
      "AI:  \n",
      "\n",
      "In 2020, the world experienced a pandemic caused by the novel coronavirus (COVID-19). The virus spread rapidly and caused widespread illness and death. Many countries implemented lockdowns and other measures to try to slow the spread of the virus. The pandemic also had a significant impact on the global economy, causing widespread job losses and business closures. Additionally, there were protests and unrest in many parts of the world, including the United States, where there were widespread protests against the killing of George Floyd.\n",
      "\n",
      "\n",
      "New human question: hi\n",
      "Response:\"\n",
      "Output [Text 0 Beam 0]: \"Hello! How can I help you today?\"\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "hi  Hello! How can I help you today?\n",
      "\n",
      "INFO:     127.0.0.1:52480 - \"POST /chat HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52474 - \"GET /user.png HTTP/1.1\" 404 Not Found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [16195]\n"
     ]
    }
   ],
   "source": [
    "app = FastAPI()\n",
    "# Allow CORS for all origins, methods, and headers\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"GET\", \"POST\", \"OPTIONS\"],  # Allow OPTIONS method\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Mount the static directory to serve static files\n",
    "app.mount(\"/static\", StaticFiles(directory=\"/home/adebolajo/Desktop/uniccon/ocr-open-source/OCR_LLM_CHATBOT/backend/static\"), name=\"static\")\n",
    "\n",
    "# Create a Jinja2Templates instance for rendering HTML templates\n",
    "templates = Jinja2Templates(directory=\"/home/adebolajo/Desktop/uniccon/ocr-open-source/OCR_LLM_CHATBOT/backend/templates\")\n",
    "\n",
    "@app.get(\"/\",response_class=HTMLResponse)\n",
    "async def read_root(request: Request):\n",
    "    return templates.TemplateResponse(\"fred.html\", {\"request\": request})\n",
    "\n",
    "\n",
    "@app.post(\"/chat\")\n",
    "async def chat_completion(message: Message):\n",
    "    # Your logic to generate a response based on the received message\n",
    "    response = \"This is a sample response.\"\n",
    "    msg = message.content\n",
    "    response = conversation({\"question\": msg})\n",
    "    print( msg, response['text'])\n",
    "\n",
    "\n",
    "    #return {\"response\": \"Sure, I can help you with that. Here's a simple FastAPI code that you can use as a starting point:\\n```python\\nfrom fastapi import FastAPI\\n\\napp = FastAPI()\\n\\n@app.get(\\\"/\\\")\\nasync def read_root():\\n    return {\\\"Hello\\\": \\\"World\\\"}\\n```\\nThis code creates a FastAPI application and defines a route for the root endpoint. When you run this code, you can access the root endpoint by visiting `http://localhost:8000/` in your web browser.\\nIs there anything else you need help with?\"}\n",
    "    return {\"response\": response['text']}\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = uvicorn.Config(app,host=\"0.0.0.0\", port=8000)\n",
    "    server = uvicorn.Server(config)\n",
    "    await server.serve()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
